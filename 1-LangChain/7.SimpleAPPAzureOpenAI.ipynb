{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14064d30",
   "metadata": {},
   "source": [
    "# Simple Gen AI APP Using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54fa56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.langchain.com/langsmith/prompt-engineering-quickstart\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "open_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "open_endpoint = os.getenv(\"OPENAI_API_END\")\n",
    "# langsmith tracking\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f32c34c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Ingestion - Web Scrapping\n",
    "from langchain_community.document_loaders import WebBaseLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b0d319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.document_loaders.web_base.WebBaseLoader object at 0x000001F2F9ACDFD0>\n"
     ]
    }
   ],
   "source": [
    "loader = WebBaseLoader(['https://docs.langchain.com/langsmith/prompt-engineering-quickstart'])\n",
    "print(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99822c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/prompt-engineering-quickstart', 'title': 'Prompt engineering quickstart - Docs by LangChain', 'language': 'en'}, page_content='Prompt engineering quickstart - Docs by LangChainOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KAsk AIForumForumSearch...NavigationQuickstartsPrompt engineering quickstartGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationForumOn this pageSDK1. Setup2. Create a prompt3. Test a prompt4. Iterate on a prompt5. Next stepsUI1. Setup2. Create a prompt3. Test a prompt4. Save a prompt5. Iterate on a prompt6. Next stepsQuickstartsPrompt engineering quickstartCopy pageCopy pageWhile traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith gives you tools to iterate, version, and collaborate on prompts so you can continuously improve your application.\\nThis guide will walk through how to create, test, and iterate on prompts using the SDK and in the UI. In this guide we will use OpenAI, but you can also use other LLM providers.\\n\\u200bSDK\\n\\u200b1. Setup\\nFirst, install the required packages:\\nPythonTypeScriptCopyAsk AIpip install -qU langsmith openai langchain_core\\n\\nNext, make sure you have signed up for a LangSmith account, then create and set your API key. You will also want to sign up for an OpenAI API key to run the code in this tutorial.\\nCopyAsk AILANGSMITH_API_KEY = \\'<your_api_key>\\'\\nOPENAI_API_KEY = \\'<your_api_key>\\'\\n\\n\\u200b2. Create a prompt\\nTo create a prompt in LangSmith, define the list of messages you want in your prompt and then wrap them using the ChatPromptTemplate function (Python) or TypeScript function. Then all you have to do is call push_prompt (Python) or pushPrompt (TypeScript) to send your prompt to LangSmith!\\nPythonTypeScriptCopyAsk AIfrom langsmith import Client\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\n# Connect to the LangSmith client\\nclient = Client()\\n\\n# Define the prompt\\nprompt = ChatPromptTemplate([\\n    (\"system\", \"You are a helpful chatbot.\"),\\n    (\"user\", \"{question}\"),\\n])\\n\\n# Push the prompt\\nclient.push_prompt(\"my-prompt\", object=prompt)\\n\\n\\u200b3. Test a prompt\\nTo test a prompt, you need to pull the prompt, invoke it with the input values you want to test and then call the model with those input values. your LLM or application expects.\\nPythonTypeScriptCopyAsk AIfrom langsmith import Client\\nfrom openai import OpenAI\\nfrom langchain_core.messages import convert_to_openai_messages\\n\\n# Connect to LangSmith and OpenAI\\nclient = Client()\\noai_client = OpenAI()\\n\\n# Pull the prompt to use\\n# You can also specify a specific commit by passing the commit hash \"my-prompt:<commit-hash>\"\\nprompt = client.pull_prompt(\"my-prompt\")\\n\\n# Since our prompt only has one variable we could also pass in the value directly\\n# The code below is equivalent to formatted_prompt = prompt.invoke(\"What is the color of the sky?\")\\nformatted_prompt = prompt.invoke({\"question\": \"What is the color of the sky?\"})\\n\\n# Test the prompt\\nresponse = oai_client.chat.completions.create(\\n    model=\"gpt-4o\",\\n    messages=convert_to_openai_messages(formatted_prompt.messages),\\n)\\n\\n\\u200b4. Iterate on a prompt\\nLangSmith makes it easy to iterate on prompts with your entire team. Members of your workspace can select a prompt to iterate on, and once they are happy with their changes, they can simply save it as a new commit.\\nTo improve your prompts:\\n\\n\\nWe recommend referencing the documentation provided by your model provider for best practices in prompt creation, such as Best practices for prompt engineering with the OpenAI API and Gemini’s Introduction to prompt design.\\n\\n\\nTo help with iterating on your prompts in LangSmith, we’ve created Prompt Canvas — an interactive tool to build and optimize your prompts. Learn about how to use Prompt Canvas.\\n\\n\\nTo add a new commit to a prompt, you can use the same push_prompt (Python) or pushPrompt (TypeScript) methods as when you first created the prompt.\\nPythonTypeScriptCopyAsk AIfrom langsmith import Client\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\n# Connect to the LangSmith client\\nclient = Client()\\n\\n# Define the prompt to update\\nnew_prompt = ChatPromptTemplate([\\n    (\"system\", \"You are a helpful chatbot. Respond in Spanish.\"),\\n    (\"user\", \"{question}\"),\\n])\\n\\n# Push the updated prompt making sure to use the correct prompt name\\n# Tags can help you remember specific versions in your commit history\\nclient.push_prompt(\"my-prompt\", object=new_prompt, tags=[\"Spanish\"])\\n\\n\\u200b5. Next steps\\n\\nLearn more about how to store and manage prompts using the Prompt Hub in these how-to guides\\nLearn more about how to use the playground for prompt engineering in these how-to guides\\n\\n\\u200bUI\\nThis quick start will walk through how to create, test, and iterate on prompts in LangSmith.\\nThis tutorial uses the UI for prompt engineering, if you are interested in using the SDK instead, read this guide.\\n\\u200b1. Setup\\nThe only setup needed for this guide is to make sure you have signed up for a LangSmith account.\\n\\u200b2. Create a prompt\\nTo create a prompt in LangSmith, navigate to the Prompts section of the left-hand sidebar and click on the ”+ New Prompt” button. You can then modify the prompt by editing/adding messages and input variables.\\n\\n\\u200b3. Test a prompt\\nTo test a prompt, set the model configuration you want to use, add your LLM provider’s API key, specify the prompt input values you want to test, and then click “Start”.\\nTo learn about more options for configuring your prompt in the playground, check out this guide. If you are interested in testing how your prompt performs over a dataset instead of individual examples, read this page.\\n\\n\\u200b4. Save a prompt\\nOnce you have run some tests and made your desired changes to your prompt you can click the “Save” button to save your prompt for future use.\\n\\n\\u200b5. Iterate on a prompt\\nLangSmith makes it easy to iterate on prompts with your entire team. Members of your workspace can select a prompt to iterate on in the playground, and once they are happy with their changes, they can simply save it as a new commit.\\nTo improve your prompts:\\n\\n\\nWe recommend referencing the documentation provided by your model provider for best practices in prompt creation, such as Best practices for prompt engineering with the OpenAI API and Gemini’s Introduction to prompt design.\\n\\n\\nTo help with iterating on your prompts in LangSmith, we’ve created Prompt Canvas — an interactive tool to build and optimize your prompts. Learn about how to use Prompt Canvas.\\n\\n\\n\\nYou can also tag specific commits to mark important moments in your commit history:\\n\\n\\u200b6. Next steps\\n\\nLearn more about how to store and manage prompts using the Prompt Hub in these how-to guides\\nLearn more about how to use the playground for prompt engineering in these how-to guides\\nEvaluate an applicationAPI referenceAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]\n"
     ]
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d3d11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.langchain.com/langsmith/prompt-engineering-quickstart', 'title': 'Prompt engineering quickstart - Docs by LangChain', 'language': 'en'}, page_content='Prompt engineering quickstart - Docs by LangChainOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KAsk AIForumForumSearch...NavigationQuickstartsPrompt engineering quickstartGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationForumOn this pageSDK1. Setup2. Create a prompt3. Test a prompt4. Iterate on a prompt5. Next stepsUI1. Setup2. Create a prompt3. Test a prompt4. Save a prompt5. Iterate on a prompt6. Next stepsQuickstartsPrompt engineering quickstartCopy pageCopy pageWhile'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/prompt-engineering-quickstart', 'title': 'Prompt engineering quickstart - Docs by LangChain', 'language': 'en'}, page_content='Iterate on a prompt6. Next stepsQuickstartsPrompt engineering quickstartCopy pageCopy pageWhile traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith gives you tools to iterate, version, and collaborate on prompts so you can continuously improve your application.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/prompt-engineering-quickstart', 'title': 'Prompt engineering quickstart - Docs by LangChain', 'language': 'en'}, page_content='This guide will walk through how to create, test, and iterate on prompts using the SDK and in the UI. In this guide we will use OpenAI, but you can also use other LLM providers.\\n\\u200bSDK\\n\\u200b1. Setup\\nFirst, install the required packages:\\nPythonTypeScriptCopyAsk AIpip install -qU langsmith openai langchain_core'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/prompt-engineering-quickstart', 'title': 'Prompt engineering quickstart - Docs by LangChain', 'language': 'en'}, page_content='Next, make sure you have signed up for a LangSmith account, then create and set your API key. You will also want to sign up for an OpenAI API key to run the code in this tutorial.\\nCopyAsk AILANGSMITH_API_KEY = \\'<your_api_key>\\'\\nOPENAI_API_KEY = \\'<your_api_key>\\'\\n\\n\\u200b2. Create a prompt\\nTo create a prompt in LangSmith, define the list of messages you want in your prompt and then wrap them using the ChatPromptTemplate function (Python) or TypeScript function. Then all you have to do is call push_prompt (Python) or pushPrompt (TypeScript) to send your prompt to LangSmith!\\nPythonTypeScriptCopyAsk AIfrom langsmith import Client\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\n# Connect to the LangSmith client\\nclient = Client()\\n\\n# Define the prompt\\nprompt = ChatPromptTemplate([\\n    (\"system\", \"You are a helpful chatbot.\"),\\n    (\"user\", \"{question}\"),\\n])\\n\\n# Push the prompt\\nclient.push_prompt(\"my-prompt\", object=prompt)'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/prompt-engineering-quickstart', 'title': 'Prompt engineering quickstart - Docs by LangChain', 'language': 'en'}, page_content='# Push the prompt\\nclient.push_prompt(\"my-prompt\", object=prompt)\\n\\n\\u200b3. Test a prompt\\nTo test a prompt, you need to pull the prompt, invoke it with the input values you want to test and then call the model with those input values. your LLM or application expects.\\nPythonTypeScriptCopyAsk AIfrom langsmith import Client\\nfrom openai import OpenAI\\nfrom langchain_core.messages import convert_to_openai_messages\\n\\n# Connect to LangSmith and OpenAI\\nclient = Client()\\noai_client = OpenAI()\\n\\n# Pull the prompt to use\\n# You can also specify a specific commit by passing the commit hash \"my-prompt:<commit-hash>\"\\nprompt = client.pull_prompt(\"my-prompt\")\\n\\n# Since our prompt only has one variable we could also pass in the value directly\\n# The code below is equivalent to formatted_prompt = prompt.invoke(\"What is the color of the sky?\")\\nformatted_prompt = prompt.invoke({\"question\": \"What is the color of the sky?\"})'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/prompt-engineering-quickstart', 'title': 'Prompt engineering quickstart - Docs by LangChain', 'language': 'en'}, page_content='# Test the prompt\\nresponse = oai_client.chat.completions.create(\\n    model=\"gpt-4o\",\\n    messages=convert_to_openai_messages(formatted_prompt.messages),\\n)\\n\\n\\u200b4. Iterate on a prompt\\nLangSmith makes it easy to iterate on prompts with your entire team. Members of your workspace can select a prompt to iterate on, and once they are happy with their changes, they can simply save it as a new commit.\\nTo improve your prompts:\\n\\n\\nWe recommend referencing the documentation provided by your model provider for best practices in prompt creation, such as Best practices for prompt engineering with the OpenAI API and Gemini’s Introduction to prompt design.\\n\\n\\nTo help with iterating on your prompts in LangSmith, we’ve created Prompt Canvas — an interactive tool to build and optimize your prompts. Learn about how to use Prompt Canvas.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/prompt-engineering-quickstart', 'title': 'Prompt engineering quickstart - Docs by LangChain', 'language': 'en'}, page_content='To add a new commit to a prompt, you can use the same push_prompt (Python) or pushPrompt (TypeScript) methods as when you first created the prompt.\\nPythonTypeScriptCopyAsk AIfrom langsmith import Client\\nfrom langchain_core.prompts import ChatPromptTemplate\\n\\n# Connect to the LangSmith client\\nclient = Client()\\n\\n# Define the prompt to update\\nnew_prompt = ChatPromptTemplate([\\n    (\"system\", \"You are a helpful chatbot. Respond in Spanish.\"),\\n    (\"user\", \"{question}\"),\\n])\\n\\n# Push the updated prompt making sure to use the correct prompt name\\n# Tags can help you remember specific versions in your commit history\\nclient.push_prompt(\"my-prompt\", object=new_prompt, tags=[\"Spanish\"])\\n\\n\\u200b5. Next steps\\n\\nLearn more about how to store and manage prompts using the Prompt Hub in these how-to guides\\nLearn more about how to use the playground for prompt engineering in these how-to guides'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/prompt-engineering-quickstart', 'title': 'Prompt engineering quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200bUI\\nThis quick start will walk through how to create, test, and iterate on prompts in LangSmith.\\nThis tutorial uses the UI for prompt engineering, if you are interested in using the SDK instead, read this guide.\\n\\u200b1. Setup\\nThe only setup needed for this guide is to make sure you have signed up for a LangSmith account.\\n\\u200b2. Create a prompt\\nTo create a prompt in LangSmith, navigate to the Prompts section of the left-hand sidebar and click on the ”+ New Prompt” button. You can then modify the prompt by editing/adding messages and input variables.\\n\\n\\u200b3. Test a prompt\\nTo test a prompt, set the model configuration you want to use, add your LLM provider’s API key, specify the prompt input values you want to test, and then click “Start”.\\nTo learn about more options for configuring your prompt in the playground, check out this guide. If you are interested in testing how your prompt performs over a dataset instead of individual examples, read this page.'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/prompt-engineering-quickstart', 'title': 'Prompt engineering quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200b4. Save a prompt\\nOnce you have run some tests and made your desired changes to your prompt you can click the “Save” button to save your prompt for future use.\\n\\n\\u200b5. Iterate on a prompt\\nLangSmith makes it easy to iterate on prompts with your entire team. Members of your workspace can select a prompt to iterate on in the playground, and once they are happy with their changes, they can simply save it as a new commit.\\nTo improve your prompts:\\n\\n\\nWe recommend referencing the documentation provided by your model provider for best practices in prompt creation, such as Best practices for prompt engineering with the OpenAI API and Gemini’s Introduction to prompt design.\\n\\n\\nTo help with iterating on your prompts in LangSmith, we’ve created Prompt Canvas — an interactive tool to build and optimize your prompts. Learn about how to use Prompt Canvas.\\n\\n\\n\\nYou can also tag specific commits to mark important moments in your commit history:\\n\\n\\u200b6. Next steps'),\n",
       " Document(metadata={'source': 'https://docs.langchain.com/langsmith/prompt-engineering-quickstart', 'title': 'Prompt engineering quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200b6. Next steps\\n\\nLearn more about how to store and manage prompts using the Prompt Hub in these how-to guides\\nLearn more about how to use the playground for prompt engineering in these how-to guides\\nEvaluate an applicationAPI referenceAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ab8e7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AzureOpenAIEmbeddings(client=<openai.resources.embeddings.Embeddings object at 0x000001F2FBF10500>, async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001F2FC3F8200>, model='text-embedding-ada-002', dimensions=None, deployment=None, openai_api_version='2024-12-01-preview', openai_api_base=None, openai_api_type='azure', openai_proxy=None, embedding_ctx_length=8191, openai_api_key=SecretStr('**********'), openai_organization=None, allowed_special=None, disallowed_special=None, chunk_size=2048, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None, http_async_client=None, check_embedding_ctx_length=True, azure_endpoint='https://cortizh2-0745-resource.services.ai.azure.com/', azure_ad_token=None, azure_ad_token_provider=None, azure_ad_async_token_provider=None, validate_base_url=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    azure_endpoint=open_endpoint,\n",
    "    api_key=open_key,\n",
    "    api_version=\"2024-12-01-preview\"\n",
    ")\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e7a1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vectorstoredb = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8665a949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x1f2ffd182c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstoredb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75c0df69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This guide will walk through how to create, test, and iterate on prompts using the SDK and in the UI. In this guide we will use OpenAI, but you can also use other LLM providers.\n",
      "​SDK\n",
      "​1. Setup\n",
      "First, install the required packages:\n",
      "PythonTypeScriptCopyAsk AIpip install -qU langsmith openai langchain_core\n"
     ]
    }
   ],
   "source": [
    "query = \"The first thing you might want to trace is all your OpenAI calls\"\n",
    "results = vectorstoredb.similarity_search(query)\n",
    "print(results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b95308b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    api_key=open_key,\n",
    "    azure_endpoint=open_endpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2ca7513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\n    Answer the following question {input}, based only on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n'), additional_kwargs={})])\n",
       "| AzureChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001F2C7505A30>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F2C73ACFB0>, root_client=<openai.lib.azure.AzureOpenAI object at 0x000001F2C737CC20>, root_async_client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F2C7504B60>, model_name='gpt-5-nano', model_kwargs={}, openai_api_key=SecretStr('**********'), disabled_params={'parallel_tool_calls': None}, azure_endpoint='https://cortizh2-0745-resource.services.ai.azure.com/', openai_api_version='2024-12-01-preview', openai_api_type='azure')\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieval Chain > Document Chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Answer the following question {input}, based only on the provided context:\n",
    "    <context>\n",
    "    {context}\n",
    "    </context>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18f8213c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All of your OpenAI calls.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document_chain.invoke({\n",
    "    \"input\": \"The first thing you might want to trace is all your OpenAI calls\",\n",
    "    \"context\": [Document(page_content=\"\"\"The first thing you might want to trace is all your OpenAI calls this guide will walk through how to create, test, and iterate on prompts using the SDK and in the UI. In this guide we will use OpenAI, but you can also use other LLM providers.\n",
    "​SDK\n",
    "​1. Setup\n",
    "First, install the required packages:\n",
    "PythonTypeScriptCopyAsk AIpip install -qU langsmith openai langchain_core\"\"\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4faad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "## Input > Retriever < Vector Store DB\n",
    "retriever = vectorstoredb.as_retriever()\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ccf5c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'AzureOpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001F2FFD182C0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\n    Answer the following question {input}, based only on the provided context:\\n    <context>\\n    {context}\\n    </context>\\n'), additional_kwargs={})])\n",
       "            | AzureChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000001F2C7505A30>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000001F2C73ACFB0>, root_client=<openai.lib.azure.AzureOpenAI object at 0x000001F2C737CC20>, root_async_client=<openai.lib.azure.AsyncAzureOpenAI object at 0x000001F2C7504B60>, model_name='gpt-5-nano', model_kwargs={}, openai_api_key=SecretStr('**********'), disabled_params={'parallel_tool_calls': None}, azure_endpoint='https://cortizh2-0745-resource.services.ai.azure.com/', openai_api_version='2024-12-01-preview', openai_api_type='azure')\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c69f171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'The first thing you might want to trace is all your OpenAI calls', 'context': [Document(id='63c67ad6-0b38-4524-84df-8703a250c3c7', metadata={'source': 'https://docs.langchain.com/langsmith/prompt-engineering-quickstart', 'title': 'Prompt engineering quickstart - Docs by LangChain', 'language': 'en'}, page_content='This guide will walk through how to create, test, and iterate on prompts using the SDK and in the UI. In this guide we will use OpenAI, but you can also use other LLM providers.\\n\\u200bSDK\\n\\u200b1. Setup\\nFirst, install the required packages:\\nPythonTypeScriptCopyAsk AIpip install -qU langsmith openai langchain_core'), Document(id='8f42901a-6b4f-42fc-94bd-b4c44e2166e4', metadata={'source': 'https://docs.langchain.com/langsmith/prompt-engineering-quickstart', 'title': 'Prompt engineering quickstart - Docs by LangChain', 'language': 'en'}, page_content='Prompt engineering quickstart - Docs by LangChainOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KLangSmithPlatform for LLM observability and evaluationOverviewQuickstartsTrace an applicationEvaluate an applicationTest promptsAPI & SDKsAPI referencePython SDKJS/TS SDKPricingPlansPricing FAQOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.Docs by LangChain home pagePythonSearch...⌘KAsk AIForumForumSearch...NavigationQuickstartsPrompt engineering quickstartGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationGet startedObservabilityEvaluationPrompt engineeringSelf-hostingAdministrationForumOn this pageSDK1. Setup2. Create a prompt3. Test a prompt4. Iterate on a prompt5. Next stepsUI1. Setup2. Create a prompt3. Test a prompt4. Save a prompt5. Iterate on a prompt6. Next stepsQuickstartsPrompt engineering quickstartCopy pageCopy pageWhile'), Document(id='174f2c62-5bf1-419e-b6ab-782967362f03', metadata={'source': 'https://docs.langchain.com/langsmith/prompt-engineering-quickstart', 'title': 'Prompt engineering quickstart - Docs by LangChain', 'language': 'en'}, page_content='\\u200b6. Next steps\\n\\nLearn more about how to store and manage prompts using the Prompt Hub in these how-to guides\\nLearn more about how to use the playground for prompt engineering in these how-to guides\\nEvaluate an applicationAPI referenceAssistantResponses are generated using AI and may contain mistakes.Docs by LangChain home pagegithubxlinkedinyoutubeResourcesChangelogLangChain AcademyTrust CenterCompanyAboutCareersBloggithubxlinkedinyoutubePowered by Mintlify'), Document(id='87530948-9301-4b08-bae4-bdd6f23fdccf', metadata={'source': 'https://docs.langchain.com/langsmith/prompt-engineering-quickstart', 'title': 'Prompt engineering quickstart - Docs by LangChain', 'language': 'en'}, page_content='Iterate on a prompt6. Next stepsQuickstartsPrompt engineering quickstartCopy pageCopy pageWhile traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith gives you tools to iterate, version, and collaborate on prompts so you can continuously improve your application.')], 'answer': \"Based on the provided context, the first thing to trace is every OpenAI call. Here's how to approach it using what's in the context:\\n\\n- Install and set up for tracing\\n  - Install LangSmith and OpenAI (as shown in the SDK setup).\\n  - Use the SDK/UI workflow described in the prompt engineering quickstart to connect your app to OpenAI.\\n\\n- Instrument every OpenAI interaction\\n  - Ensure each prompt sent to OpenAI and the corresponding response from OpenAI are captured as trace events.\\n  - Do this for all OpenAI calls your app makes via the SDK/UI.\\n\\n- Use LangSmith observability and evaluation\\n  - Leverage LangSmith to trace, evaluate, and iterate on prompts during development and testing.\\n\\n- Follow the prompt engineering workflow\\n  - Create a prompt, test a prompt, and iterate on a prompt as outlined in the quickstart.\\n  - In the UI, you can save and re-test prompts as you refine them.\\n\\n- Next steps after tracing\\n  - Learn how to store and manage prompts with the Prompt Hub for ongoing prompt management.\\n  - Use the playground and other LangChain/LangSmith resources to further evaluate and improve prompts.\\n\\nIn short: enable LangSmith, log every OpenAI prompt and its response, and use the prompt engineering workflow (create/test/iterate) to continuously improve while tracing.\"}\n"
     ]
    }
   ],
   "source": [
    "# Get Response from the llm\n",
    "\n",
    "response = retrieval_chain.invoke({\"input\":\"The first thing you might want to trace is all your OpenAI calls\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6a03df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, the first thing to trace is every OpenAI call. Here's how to approach it using what's in the context:\n",
      "\n",
      "- Install and set up for tracing\n",
      "  - Install LangSmith and OpenAI (as shown in the SDK setup).\n",
      "  - Use the SDK/UI workflow described in the prompt engineering quickstart to connect your app to OpenAI.\n",
      "\n",
      "- Instrument every OpenAI interaction\n",
      "  - Ensure each prompt sent to OpenAI and the corresponding response from OpenAI are captured as trace events.\n",
      "  - Do this for all OpenAI calls your app makes via the SDK/UI.\n",
      "\n",
      "- Use LangSmith observability and evaluation\n",
      "  - Leverage LangSmith to trace, evaluate, and iterate on prompts during development and testing.\n",
      "\n",
      "- Follow the prompt engineering workflow\n",
      "  - Create a prompt, test a prompt, and iterate on a prompt as outlined in the quickstart.\n",
      "  - In the UI, you can save and re-test prompts as you refine them.\n",
      "\n",
      "- Next steps after tracing\n",
      "  - Learn how to store and manage prompts with the Prompt Hub for ongoing prompt management.\n",
      "  - Use the playground and other LangChain/LangSmith resources to further evaluate and improve prompts.\n",
      "\n",
      "In short: enable LangSmith, log every OpenAI prompt and its response, and use the prompt engineering workflow (create/test/iterate) to continuously improve while tracing.\n"
     ]
    }
   ],
   "source": [
    "print(response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d712cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bb7ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814aba18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c9c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fee43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15eff38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd39ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
