{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d27c53b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "open_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "open_endpoint = os.getenv(\"OPENAI_API_END\")\n",
    "# langsmith tracking\n",
    "os.environ['LANGCHAIN_API_KEY'] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = \"true\"\n",
    "os.environ['LANGCHAIN_PROJECT'] = os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09748c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x0000017DC6463470> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x0000017DC5853770> root_client=<openai.lib.azure.AzureOpenAI object at 0x0000017DC5853BC0> root_async_client=<openai.lib.azure.AsyncAzureOpenAI object at 0x0000017DC6460800> model_name='gpt-5-nano' model_kwargs={} openai_api_key=SecretStr('**********') disabled_params={'parallel_tool_calls': None} azure_endpoint='https://cortizh2-0745-resource.services.ai.azure.com/' openai_api_version='2024-12-01-preview' openai_api_type='azure'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(api_version=\"2024-12-01-preview\",\n",
    "                      azure_endpoint=open_endpoint, api_key=open_key,\n",
    "                      model=\"gpt-5-nano\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32d65fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Generative AI is a type of artificial intelligence that can create new content—things like text, images, music, code, or video—by learning patterns from lots of existing data. Instead of just recognizing or classifying things, it generates something new that resembles what it learned.\\n\\nHow it works (high level)\\n- It trains on a large dataset to learn the distribution of the data (how likely different pieces of content are).\\n- It then samples from that learned distribution to produce new content.\\n- Different families of models do this in different ways (see next section).\\n\\nCommon approaches\\n- Autoregressive models (e.g., GPT-4, other large language models): predict the next word or token given the previous ones, building up text one piece at a time.\\n- Diffusion models (e.g., Stable Diffusion, DALL·E 2): start with random noise and gradually refine it into an image or other content.\\n- Generative Adversarial Networks (GANs): a generator creates content and a discriminator tries to distinguish it from real data, improving the generator over time.\\n- Variational Autoencoders (VAEs): learn compressed representations of data and then generate new samples by decoding those representations.\\n\\nWhat it can create\\n- Text: articles, summaries, code, chat responses, translations.\\n- Images and artwork: photos, illustrations, style transfer.\\n- Music and audio: songs, sound effects.\\n- Code and software: boilerplate, snippets, or even whole programs.\\n- Other content: designs, 3D models, synthetic data for testing.\\n\\nUse cases\\n- Productivity: drafting emails, writing reports, generating code.\\n- Creativity: art generation, story ideas, music.\\n- Design and prototyping: mockups, visuals, UI ideas.\\n- Data augmentation: creating synthetic data for training other models.\\n- Accessibility: simplifying complex information into plain language or summaries.\\n\\nPros and risks\\n- Pros: faster content creation, automation of repetitive tasks, new creative possibilities, cost savings at scale.\\n- Risks: producing incorrect or misleading information (hallucinations), biases from training data, copyright and licensing concerns, privacy issues, potential misuse for deepfakes or disinformation.\\n\\nIf you want, tell me what you’re curious about (how to use it in your work, how it’s built, or what tools exist), and I can tailor the explanation or give practical examples.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1134, 'prompt_tokens': 10, 'total_tokens': 1144, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 640, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9bAzDGBRIdIr95eW379bgdBEpkff', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run--3ab95436-0e38-4abd-b10f-3b6013fa5b54-0' usage_metadata={'input_tokens': 10, 'output_tokens': 1134, 'total_tokens': 1144, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 640}}\n"
     ]
    }
   ],
   "source": [
    "# Input and get response from LLM\n",
    "\n",
    "result = llm.invoke(\"What's generative AI\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69de5aa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are and expert AI engineer. Provide answers base on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are and expert AI engineer. Provide answers base on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47d48a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangSmith is a platform from LangChain designed to help you build, test, and operate LLM-powered apps more reliably. It focuses on logging, evaluation, and reproducibility so you can understand how your prompts and chains behave in production and improve them over time.\\n\\nKey ideas and features\\n- Run logging and observability\\n  - Capture prompts, model responses, inputs, outputs, parameters, latency, and token usage\\n  - Attach artifacts (files, embeddings, screenshots, etc.) to a run\\n  - Scan and filter runs to spot patterns, failures, or regressions\\n- Reproducibility and debugging\\n  - Trace and visualize the flow of calls across a chain or multi-step workflow\\n  - Replay or “play back” runs to reproduce issues and see exact inputs/outputs\\n- Evaluation and quality measurement\\n  - Built-in evaluation capabilities to compare outputs against criteria or rubrics\\n  - Support for human-in-the-loop feedback and custom evaluation metrics\\n  - Side-by-side comparison of different prompts, models, or configurations\\n- Observability and dashboards\\n  - Dashboards and search to monitor latency, error rates, cost (token usage), and overall quality\\n  - Helpful when doing prompt engineering, model selection, or A/B testing\\n- Ecosystem and workflow integration\\n  - Tight integration with LangChain workflows and LLM calls\\n  - Works with Python (and related LangChain tooling) and is designed to fit typical MLOps workflows\\n\\nTypical use cases\\n- Prompt engineering and prompt-safe testing: compare different prompts and templates, and see which yields better quality or fewer errors\\n- Model benchmarking and A/B testing: evaluate multiple models or configurations side by side\\n- Debugging production issues: trace a failing run, inspect inputs/outputs, and reproduce it\\n- Compliance and governance: maintain a traceable record of how the model was used and evaluated\\n\\nGetting started (high level)\\n- Sign up for LangSmith and obtain an API key for your project\\n- Install and instrument your code (usually via the LangSmith SDK)\\n  - Log runs, inputs, outputs, and artifacts as you call LLMs or run chains\\n  - Optionally tag runs with metadata (model version, task type, environment)\\n- Use the LangSmith UI or dashboards to search, compare, and replay runs\\n- Add or configure evaluations and human feedback as needed\\n\\nIf you share your tech stack (Python vs. Node, LangChain usage, whether you’re focusing on evaluation, debuggability, or governance), I can give you a tailored overview and a simple setup plan or sample code to get started.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 2016, 'prompt_tokens': 31, 'total_tokens': 2047, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1472, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-C9bHhFnEg4LYr7PulUbZh52eYG59B', 'service_tier': None, 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='run--368e9ae7-affe-40f2-a189-3ed539143fbb-0' usage_metadata={'input_tokens': 31, 'output_tokens': 2016, 'total_tokens': 2047, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1472}}\n"
     ]
    }
   ],
   "source": [
    "chain = prompt|llm\n",
    "\n",
    "response = chain.invoke({\"input\":\"Can you tell me about langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12ea851a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stroutput Parser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt|llm|output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "590a0fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " LangSmith is LangChain’s observability, debugging, and evaluation platform for LLM-powered apps. It helps you instrument, log, analyze, and compare how your prompts and models behave in production or in development.\n",
      "\n",
      "Key ideas and features\n",
      "- End-to-end logging: capture prompts, model outputs, tool calls, memory states, and other relevant metadata for each run or step in a workflow.\n",
      "- Run and event tracing: organize activity into Runs (high-level sessions) and Events (individual actions like an LLM call or tool invocation) so you can replay or inspect exactly what happened.\n",
      "- Observability and analytics: view statistics, latency, token usage, failure rates, and other metrics to understand performance and reliability.\n",
      "- Prompt and model comparisons: compare different prompts, prompts templates, or models side-by-side to see which yields better results.\n",
      "- Debugging and reproduction: reproduce runs to diagnose issues, reproduce bugs, or verify fixes.\n",
      "- Human-in-the-loop evaluation: incorporate manual scoring or critique alongside automated signals to improve quality.\n",
      "- Integrations: designed to work with LangChain workflows, but usable with any LLM-driven app. You can log data from custom code, not just from LangChain pipelines.\n",
      "- Privacy and control: you decide what to log; you can redact sensitive information and manage data retention.\n",
      "\n",
      "Typical use cases\n",
      "- Debugging failing or unexpected LLM outputs by inspecting inputs, prompts, tool invocations, and responses.\n",
      "- Auditing and compliance: keeping an auditable trail of how decisions were made by the model.\n",
      "- Model comparison and A/B testing: testing multiple prompts, configurations, or models to see which performs best on your tasks.\n",
      "- Production monitoring: tracking latency, errors, and usage patterns in live deployments.\n",
      "- Evaluation and QA: setting up automated or human-in-the-loop scoring to improve quality and align with goals.\n",
      "\n",
      "Getting started at a high level\n",
      "- Sign up for LangSmith and get an API key.\n",
      "- Install the LangSmith SDK in your project.\n",
      "- Initialize the LangSmith client in your application and point it to your project/workspace.\n",
      "- Instrument your code to log Runs and Events around LLM calls, tool uses, and other relevant steps.\n",
      "- Run your app and push data to LangSmith; use the UI to explore runs, compare configurations, and identify issues.\n",
      "- Optionally integrate with LangChain to get tighter coupling with chains, prompts, and memory components.\n",
      "\n",
      "What to consider\n",
      "- Data you log: only log what you need. Be mindful of PII or sensitive data; use redaction or selective logging when necessary.\n",
      "- Costs and data retention: understand the pricing and retention policies for stored logs and analyses.\n",
      "- Privacy and security: ensure your deployment complies with your organization's policies when sending data to an external service.\n",
      "\n",
      "Where to learn more\n",
      "- LangSmith docs and getting-started guides (official docs typically at docs.langsmith.com or via LangChain’s site).\n",
      "- Tutorials that show logging LLM calls, prompts, and model outputs, plus how to compare prompts and models.\n",
      "- If you’re using LangChain, look for the LangChain + LangSmith integration guides for a smoother setup.\n",
      "\n",
      "If you tell me your tech stack (Python vs. Node, LangChain usage, cloud environment, etc.) I can tailor a concrete getting-started plan and point you to the exact commands and API usage.\n"
     ]
    }
   ],
   "source": [
    "response = chain.invoke({\"input\":\"Can you tell me about langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b532596d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
